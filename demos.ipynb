{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import nan\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "import os, glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "image registration and cropping functions\n",
    "\"\"\"\n",
    "def align_to_standard(img_dir, src_dir='src.jpeg'):\n",
    "    src_color = cv2.imread(src_dir)\n",
    "    sbj_color = cv2.imread(img_dir)\n",
    "\n",
    "    src = cv2.cvtColor(src_color, cv2.COLOR_BGR2GRAY)\n",
    "    sbj = cv2.cvtColor(sbj_color, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    orb_detector = cv2.ORB_create(500)\n",
    "    src_keypoints, src_descriptors = orb_detector.detectAndCompute(src, None)\n",
    "    sbj_keypoints, sbj_descriptors = orb_detector.detectAndCompute(sbj,None)\n",
    "\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    matches = list(matcher.match(sbj_descriptors, src_descriptors))\n",
    "    \n",
    "    matches.sort(key = lambda x: x.distance)\n",
    "    matches = matches[:int(len(matches)*0.9)]\n",
    "    # records matched keypoints' coordinates\n",
    "    no_of_matches = len(matches)\n",
    "    p1 = np.zeros((no_of_matches, 2))\n",
    "    p2 = np.zeros((no_of_matches, 2))\n",
    "\n",
    "    for i in range(no_of_matches):\n",
    "        p1[i, :] = sbj_keypoints[matches[i].queryIdx].pt\n",
    "        p2[i, :] = src_keypoints[matches[i].trainIdx].pt\n",
    "    \n",
    "    homography, mask = cv2.findHomography(p1, p2, cv2.RANSAC)\n",
    "    transformed_img = cv2.warpPerspective(sbj_color,\n",
    "                        homography, (src.shape[1], src.shape[0]))\n",
    "    return transformed_img\n",
    "\n",
    "def crop_rotate_dir(img, og_file_dir, output_dir, angle = -1.5, left = 135, upper =85, right = 600, lower = 390):\n",
    "    filename = og_file_dir.split('/')[-1]\n",
    "    im = Image.fromarray(img)\n",
    "    rotated = im.rotate(angle, expand = 1)\n",
    "    im_final = rotated.crop((left, upper, right, lower))            \n",
    "    im_final.save(output_dir+'/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input: filename of plate\n",
    "output: pd df of the ground truth for that plate\n",
    "\"\"\"\n",
    "from typing import Tuple\n",
    "\n",
    "all_labels = pd.read_csv('updated_ground_truth.csv')\n",
    "\n",
    "def get_ground_truth(filename: str):\n",
    "    id = filename.split('_')[-1].split('.')[0]\n",
    "    this_labels = all_labels.loc[all_labels[\"destination_plate_bcode\"]==int(id),['destination_well']].to_numpy()\n",
    "    return this_labels\n",
    "\n",
    "\"\"\"\n",
    "gaussian_kernel_size: greater = blurring in larger neighborhood\n",
    "gaussian_sigma: greater sigma = more blurring\n",
    "adp_th_block_size needs to be odd: greater = looking at local intensities in a larger neighborhood\n",
    "adp_th_const is a constant that is subtracted from the weighted mean; greater = effectively more noise reduction\n",
    "\"\"\"\n",
    "def load_and_preprocess_img(filename: str, gaussian_kernel_size: Tuple = (3,3), gaussian_sigma: float = 1.0, adp_th_block_size: int = 5, adp_th_const: int = 4):\n",
    "    img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    img_blurred = cv2.GaussianBlur(img, gaussian_kernel_size, gaussian_sigma)\n",
    "    img_th = cv2.adaptiveThreshold(img_blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, adp_th_block_size, adp_th_const)\n",
    "    return img_th\n",
    "\n",
    "\"\"\"\n",
    "exclude wells with black background\n",
    "\"\"\"\n",
    "def exclude_wells(col_idx, row_idx):\n",
    "    if ((col_idx == 2 or col_idx == 3) and row_idx == 12) or ((row_idx ==0 or row_idx == 15) and (col_idx==11 or col_idx ==12)) or (row_idx == 2 and col_idx==20):\n",
    "        return True\n",
    "    return False\n",
    "\"\"\"\n",
    "input: img or preprocessed img\n",
    "output: conceptually a matrix of 16 x 24, each entry is the isolated well image\n",
    "\"\"\"\n",
    "def isolate_each_well(img_th):\n",
    "    to_return = []\n",
    "    y = img_th.shape[0] / 16\n",
    "    x = img_th.shape[1] / 24\n",
    "    for row in np.arange(0, img_th.shape[0], y):\n",
    "        col_out = []\n",
    "        for col in np.arange(0, img_th.shape[1], x):\n",
    "            col_out.append(img_th[round(row):round(row+y),round(col):round(col+x)])\n",
    "        to_return.append(col_out)\n",
    "    return np.array(to_return, dtype=object)\n",
    "\n",
    "\"\"\"\n",
    "If one and only one particle is detected, report 1\n",
    "If no particle detected, report 0\n",
    "else: report -1 to indicate ambiguity\n",
    "\"\"\"\n",
    "def particle_detection_prediction(well_img):\n",
    "    nb_components, arr = cv2.connectedComponents(well_img, connectivity=8)\n",
    "    if nb_components - 1 == 1:\n",
    "        pred = 1\n",
    "    elif nb_components - 1 == 0:\n",
    "        pred = 0\n",
    "    else:\n",
    "        pred = -1\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uses particle detection to predict whether or not bead is present.\n",
    "Returns accuracy and a list of unsure well locations for each img (if particle_detection_prediction returns -1)\n",
    "    which means that more than one particle is detected in the well\n",
    "\"\"\"\n",
    "from numpy import isnan\n",
    "\n",
    "def melodys_pipeline(input_dir = '../first_batch_img', cropped_output_dir = '../cropped_first_batch_img'):\n",
    "    # save aligned and cropped imgs to disk to avoid taking up too much memory\n",
    "    # for filename in glob.glob(input_dir+'/*.jpg'):\n",
    "    #     print(filename)\n",
    "    #     img = align_to_standard(filename)\n",
    "    #     crop_rotate_dir(img, filename, output_dir=cropped_output_dir,left = 135, upper =85, right = 600, lower = 390)\n",
    "\n",
    "    accuracy_list = []\n",
    "    with_label = []\n",
    "    no_label = []\n",
    "    # unsure_dict = {}\n",
    "    # read every image file from the input folder\n",
    "    for filename in glob.glob(cropped_output_dir +'/*.jpg'):\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        unsure = []\n",
    "        labels = get_ground_truth(filename)\n",
    "        if len(labels) == 0:\n",
    "            no_label.append(filename)\n",
    "            continue\n",
    "        with_label.append(filename)\n",
    "        img_th = load_and_preprocess_img(filename, (3,3), 1.0, 5, 4)\n",
    "        well_imgs = isolate_each_well(img_th)\n",
    "        for row in range(well_imgs.shape[0]):\n",
    "            for col in range(well_imgs.shape[1]):\n",
    "                if exclude_wells(col, row):\n",
    "                    continue\n",
    "                well = np.array(well_imgs[row,col])\n",
    "                # pred = particle_detection_prediction(well) # comment this if using only num of val in thresholded img and uncomment the following line\n",
    "                pred = len(np.unique(well)) - 1 \n",
    "                if pred != -1:\n",
    "                    #get truth\n",
    "                    well_id = chr(ord('A') + row) + str(col + 1)\n",
    "                    if well_id in labels:\n",
    "                        truth = 1\n",
    "                    else:\n",
    "                        truth = 0\n",
    "                        \n",
    "                    if pred == truth:\n",
    "                        right += 1\n",
    "                    else:\n",
    "                        wrong += 1\n",
    "                else:\n",
    "                    unsure.append([row, col])\n",
    "        accuracy_list.append(right / (right + wrong + len(unsure)))\n",
    "        # unsure_dict[filename] = len(unsure)\n",
    "    return accuracy_list, with_label, no_label #, unsure_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, with_label, no_labels = melodys_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../cropped_first_batch_img/220304_121253_897871.jpg',\n",
       " '../cropped_first_batch_img/220311_131245_902144.jpg',\n",
       " '../cropped_first_batch_img/220311_130343_902143.jpg',\n",
       " '../cropped_first_batch_img/220304_124800_897870.jpg',\n",
       " '../cropped_first_batch_img/220304_130557_897872.jpg',\n",
       " '../cropped_first_batch_img/220304_114822_897870.jpg',\n",
       " '../cropped_first_batch_img/220304_123855_897869.jpg',\n",
       " '../cropped_first_batch_img/220311_123951_902144.jpg',\n",
       " '../cropped_first_batch_img/220311_121952_902143.jpg',\n",
       " '../cropped_first_batch_img/220304_123254_897872.jpg',\n",
       " '../cropped_first_batch_img/220304_114444_897869.jpg',\n",
       " '../cropped_first_batch_img/220304_125653_897871.jpg',\n",
       " '../cropped_first_batch_img/220311_125438_902142.jpg',\n",
       " '../cropped_first_batch_img/220311_124536_902141.jpg',\n",
       " '../cropped_first_batch_img/220311_115935_902142.jpg',\n",
       " '../cropped_first_batch_img/220311_115450_902141.jpg']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   2.,   3.,   2.,   7.,   5.,  16.,  21.,  57., 476.]),\n",
       " array([0.25994695, 0.33395225, 0.40795756, 0.48196286, 0.55596817,\n",
       "        0.62997347, 0.70397878, 0.77798408, 0.85198939, 0.92599469,\n",
       "        1.        ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8559322033898306\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.array(accuracies)>0.9)/len(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../cropped_first_batch_img/220831_120025_1008408.jpg\n",
      "4 1\n"
     ]
    }
   ],
   "source": [
    "bad_img = np.where(np.array(accuracies)<0.7)\n",
    "to_see = np.array(with_label)[bad_img]\n",
    "total_red = 0\n",
    "bad_red = 0\n",
    "for idx, img in enumerate(with_label):\n",
    "    if '220831' in img:\n",
    "        total_red += 1\n",
    "        if img in to_see:\n",
    "            bad_red += 1\n",
    "            print(img)\n",
    "print(total_red, bad_red)\n",
    "# plt.figure(1)\n",
    "# plt.imshow(cv2.imread('../first_batch_img/220519_144906_940480.jpg'))\n",
    "# plt.figure(2)\n",
    "# plt.imshow(align_to_standard('../first_batch_img/220519_144906_940480.jpg'))\n",
    "# print(to_see[6])\n",
    "# plt.figure(3)\n",
    "# plt.imshow(cv2.imread(to_see[5]))\n",
    "# labels = get_ground_truth(to_see)\n",
    "# well_imgs = isolate_each_well(load_and_preprocess_img(to_see, (3,3), 1.0, 5, 4))\n",
    "# for row in range(len(well_imgs)):\n",
    "#     for col in range(len(well_imgs[0])):\n",
    "#         print(chr(ord('A') + row) + str(col + 1))\n",
    "#         print(len(np.unique(well_imgs[row,col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('small_val_labels.csv')\n",
    "# df.loc[(df['col_id']==1) & (df['row_id']==1),'label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def melodys_toy_data_pipeline(img_dir):\n",
    "#     accuracy_list = []\n",
    "#     unsure_dict = {}\n",
    "#     # read every image file from the input folder\n",
    "    \n",
    "#     right = 0\n",
    "#     wrong = 0\n",
    "#     unsure = []\n",
    "#     labels = pd.read_csv('small_val_labels.csv')\n",
    "#     # plt.figure(0)\n",
    "#     # plt.imshow(cv2.imread(img_dir,0))\n",
    "#     img_th = load_and_preprocess_img(img_dir, (3,3), 1.0, 5, 4)\n",
    "#     # plt.figure(999)\n",
    "#     # plt.imshow(img_th)\n",
    "#     well_imgs = isolate_each_well(img_th)\n",
    "#     for row in range(well_imgs.shape[0]):\n",
    "#         for col in range(well_imgs.shape[1]):\n",
    "#             truth = labels.loc[(labels['col_id']==col) & (labels['row_id']==row),'label'].tolist()\n",
    "#             if truth == [] or np.isnan(truth):\n",
    "#                 continue\n",
    "#             truth = truth[0]\n",
    "#             if exclude_wells(col, row):\n",
    "#                 continue\n",
    "#             well = np.array(well_imgs[row,col])\n",
    "#             # print(cv2.connectedComponents(well))\n",
    "#             pred = particle_detection_prediction(well)\n",
    "#             # pred = len(np.unique(well)) - 1\n",
    "#             # plt.figure((row+1)*(col+1))\n",
    "#             # plt.title(str(truth))\n",
    "#             # plt.imshow(well)\n",
    "#             if pred != -1:\n",
    "#                 if pred == truth:\n",
    "#                     right += 1\n",
    "#                 else:\n",
    "#                     wrong += 1\n",
    "#             else:\n",
    "#                 unsure.append([row, col])\n",
    "#     accuracy_list.append(right / (right + wrong + len(unsure)))\n",
    "#     unsure_dict[img_dir] = len(unsure)\n",
    "#     return accuracy_list, unsure_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melodys_toy_data_pipeline('220118_122237_869669.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
