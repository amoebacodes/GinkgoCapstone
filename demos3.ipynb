{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import nan\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "import os, glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "image registration and cropping functions\n",
    "\"\"\"\n",
    "def align_to_standard(img_dir, src_dir='src.jpeg'):\n",
    "    src_color = cv2.imread(src_dir)\n",
    "    sbj_color = cv2.imread(img_dir)\n",
    "\n",
    "    src = cv2.cvtColor(src_color, cv2.COLOR_BGR2GRAY)\n",
    "    sbj = cv2.cvtColor(sbj_color, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    orb_detector = cv2.ORB_create(2000)\n",
    "    src_keypoints, src_descriptors = orb_detector.detectAndCompute(src, None)\n",
    "    sbj_keypoints, sbj_descriptors = orb_detector.detectAndCompute(sbj,None)\n",
    "\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    matches = list(matcher.match(sbj_descriptors, src_descriptors))\n",
    "    \n",
    "    matches.sort(key = lambda x: x.distance)\n",
    "    matches = matches[:int(len(matches)*0.9)]\n",
    "    # records matched keypoints' coordinates\n",
    "    no_of_matches = len(matches)\n",
    "    p1 = np.zeros((no_of_matches, 2))\n",
    "    p2 = np.zeros((no_of_matches, 2))\n",
    "\n",
    "    for i in range(no_of_matches):\n",
    "        p1[i, :] = sbj_keypoints[matches[i].queryIdx].pt\n",
    "        p2[i, :] = src_keypoints[matches[i].trainIdx].pt\n",
    "    \n",
    "    homography, mask = cv2.findHomography(p1, p2, cv2.RANSAC)\n",
    "    transformed_img = cv2.warpPerspective(sbj_color,\n",
    "                        homography, (src.shape[1], src.shape[0]))\n",
    "    return transformed_img\n",
    "\n",
    "def crop_rotate_dir(img, og_file_dir, output_dir, angle = -1.5, left = 135, upper =85, right = 600, lower = 390):\n",
    "    filename = og_file_dir.split('/')[-1]\n",
    "    im = Image.fromarray(img)\n",
    "    rotated = im.rotate(angle, expand = 1)\n",
    "    im_final = rotated.crop((left, upper, right, lower))            \n",
    "    im_final.save(output_dir+'/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input: filename of plate\n",
    "output: pd df of the ground truth for that plate\n",
    "\"\"\"\n",
    "from typing import Tuple\n",
    "\n",
    "all_labels = pd.read_csv('updated_ground_truth.csv')\n",
    "\n",
    "def get_ground_truth(filename: str):\n",
    "    id = filename.split('_')[-1].split('.')[0]\n",
    "    this_labels = all_labels.loc[all_labels[\"destination_plate_bcode\"]==int(id),['destination_well']].to_numpy()\n",
    "    return this_labels\n",
    "    \n",
    "\"\"\"\n",
    "return ngs data delivery success for all wells in this plate; type = pandas df\n",
    "\"\"\"\n",
    "all_labels_w_ngs = pd.read_csv('updated_ground_truth_ngs.csv')\n",
    "def get_ground_truth_w_ngs(filename: str):\n",
    "    id = filename.split('_')[-1].split('.')[0]\n",
    "    successes = all_labels_w_ngs.loc[all_labels_w_ngs[\"destination_plate_bcode\"]==int(id), ['ngs_data_delivery_success', 'destination_well']]\n",
    "    return successes\n",
    "\n",
    "\"\"\"\n",
    "gaussian_kernel_size: greater = blurring in larger neighborhood\n",
    "gaussian_sigma: greater sigma = more blurring\n",
    "adp_th_block_size needs to be odd: greater = looking at local intensities in a larger neighborhood\n",
    "adp_th_const is a constant that is subtracted from the weighted mean; greater = effectively more noise reduction\n",
    "\"\"\"\n",
    "def load_and_preprocess_img(filename: str, gaussian_kernel_size: Tuple = (3,3), adapt=cv2.ADAPTIVE_THRESH_GAUSSIAN_C, thresh=cv2.THRESH_BINARY_INV, gaussian_sigma: float = 1.0, adp_th_block_size: int = 5, adp_th_const: int = 4):\n",
    "    img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    img_blurred = cv2.GaussianBlur(img, gaussian_kernel_size, gaussian_sigma)\n",
    "    img_th = cv2.adaptiveThreshold(img_blurred, 255, adapt, thresh, adp_th_block_size, adp_th_const)\n",
    "    return img_th\n",
    "\n",
    "\"\"\"\n",
    "exclude wells with black background\n",
    "\"\"\"\n",
    "def exclude_wells(col_idx, row_idx):\n",
    "    # M3, M4, A12, A13, P12, P13, C21\n",
    "    if ((col_idx == 2 or col_idx == 3) and row_idx == 12) or ((row_idx ==0 or row_idx == 15) and (col_idx==11 or col_idx ==12)) or (row_idx == 2 and col_idx==20):\n",
    "        return True\n",
    "    return False\n",
    "\"\"\"\n",
    "input: img or preprocessed img\n",
    "output: conceptually a matrix of 16 x 24, each entry is the isolated well image\n",
    "\"\"\"\n",
    "def isolate_each_well(img_th):\n",
    "    to_return = []\n",
    "    y = img_th.shape[0] / 16\n",
    "    x = img_th.shape[1] / 24\n",
    "    for row in np.arange(0, img_th.shape[0], y):\n",
    "        col_out = []\n",
    "        for col in np.arange(0, img_th.shape[1], x):\n",
    "            col_out.append(img_th[round(row):round(row+y),round(col):round(col+x)])\n",
    "        to_return.append(col_out)\n",
    "    return np.array(to_return, dtype=object)\n",
    "\n",
    "\"\"\"\n",
    "If one and only one particle is detected, report 1\n",
    "If no particle detected, report 0\n",
    "else: report -1 to indicate ambiguity\n",
    "\"\"\"\n",
    "def particle_detection_prediction(well_img):\n",
    "    nb_components, arr = cv2.connectedComponents(well_img, connectivity=8)\n",
    "    if nb_components - 1 == 1:\n",
    "        pred = 1\n",
    "    elif nb_components - 1 == 0:\n",
    "        pred = 0\n",
    "    else:\n",
    "        pred = -1\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uses particle detection to predict whether or not bead is present.\n",
    "Returns accuracy and a list of unsure well locations for each img (if particle_detection_prediction returns -1)\n",
    "    which means that more than one particle is detected in the well\n",
    "\"\"\"\n",
    "from numpy import isnan\n",
    "from collections import defaultdict\n",
    "\n",
    "def melodys_pipeline(thresh, adapt, block, c, k, registration, input_dir = '../second_batch_img/', cropped_output_dir = '../cropped_second_batch_img/'):\n",
    "    # save aligned and cropped imgs to disk to avoid taking up too much memory\n",
    "    # if registration:\n",
    "    #     cropped_output_dir = 'cropped2/'\n",
    "    # for filename in glob.glob(input_dir+'/*.jpg'):\n",
    "    #     # print(filename)\n",
    "    #     if registration:\n",
    "    #         img = align_to_standard(filename)\n",
    "    #     else:\n",
    "    #         img = cv2.imread(filename)\n",
    "    #     crop_rotate_dir(img, filename, output_dir=cropped_output_dir,left = 135, upper =85, right = 600, lower = 390)\n",
    "\n",
    "    accuracy_list = []\n",
    "    with_label = []\n",
    "    no_label = []\n",
    "    y_true, y_pred = [], []\n",
    "    failed_bc_dispense = 0\n",
    "    failed_not_dc_dispense = 0\n",
    "    ngs_success_counter = 0\n",
    "    ngs_failed_counter = 0\n",
    "    true_false_neg_counter = 0\n",
    "    ngs_nan = 0\n",
    "    # wells = defaultdict(lambda:0)\n",
    "    # totals = defaultdict(lambda:0)\n",
    "    # specials = defaultdict(lambda:0)\n",
    "    # unsure_dict = {}\n",
    "    # read every image file from the input folder\n",
    "    for filename in glob.glob(cropped_output_dir +'/*.jpg'):\n",
    "        print(filename)\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        unsure = []\n",
    "        labels = get_ground_truth(filename)\n",
    "        if len(labels) == 0:\n",
    "            no_label.append(filename)\n",
    "            continue\n",
    "        with_label.append(filename)\n",
    "\n",
    "        ngs_successes_df = get_ground_truth_w_ngs(filename)\n",
    "\n",
    "        img_th = load_and_preprocess_img(filename=filename, gaussian_kernel_size=(3,3), thresh=thresh, adapt=adapt, adp_th_block_size=block, adp_th_const=c, gaussian_sigma=1.0)\n",
    "        # print(type(img_th))\n",
    "        well_imgs = isolate_each_well(img_th)\n",
    "        for row in range(well_imgs.shape[0]):\n",
    "            for col in range(well_imgs.shape[1]):\n",
    "                if exclude_wells(col, row):\n",
    "                    continue\n",
    "                \n",
    "                # crop out a kxk square in the center of the well\n",
    "                well = np.array(well_imgs[row,col])\n",
    "                if well.shape[1] == 20:\n",
    "                    well = well[:, :-1]\n",
    "                row_diff = (well.shape[0] - k) // 2\n",
    "                well = well[row_diff:, :]\n",
    "                well = well[:k, :]\n",
    "                well = well[:, row_diff:]\n",
    "                well = well[:, :k]\n",
    "                \n",
    "                # pred = particle_detection_prediction(well) # comment this if using only num of val in thresholded img and uncomment the following line\n",
    "                pred = len(np.unique(well)) - 1 \n",
    "\n",
    "                #generate well id for df lookup\n",
    "                well_id = chr(ord('A') + row) + str(col + 1)\n",
    "\n",
    "                # get ngs success for this well\n",
    "                ngs_success = ngs_successes_df.loc[ngs_successes_df['destination_well'] == well_id,['ngs_data_delivery_success']].to_numpy().flatten()\n",
    "                if len(ngs_success) == 0 or ngs_success[0] == nan:\n",
    "                    ngs_success = nan\n",
    "                else:\n",
    "                    ngs_success = ngs_success[0]\n",
    "                #count ngs success\n",
    "                if ngs_success == True:\n",
    "                    ngs_success_counter += 1\n",
    "                elif ngs_success == False:\n",
    "                    ngs_failed_counter += 1\n",
    "                else:\n",
    "                    ngs_nan += 1\n",
    "\n",
    "                if pred != -1:\n",
    "                    y_pred.append(pred)\n",
    "\n",
    "                    #get truth\n",
    "                    # totals[well_id] += 1\n",
    "                    if well_id in labels:\n",
    "                        truth = 1\n",
    "                    else:\n",
    "                        truth = 0\n",
    "                    # if well_id in ['A11', 'A14'] and truth == 1:\n",
    "                    #     specials[well_id] += 1\n",
    "                    # elif well_id in ['P11', 'P14'] and truth == 1:\n",
    "                    #     specials[well_id] += 1\n",
    "                    y_true.append(truth)   \n",
    "                    if pred == truth:\n",
    "                        # wells[well_id] += 1\n",
    "                        right += 1\n",
    "\n",
    "                        if pred == 1 and ngs_success == False: # if definitely have beads but ngs not successful\n",
    "                            failed_not_dc_dispense += 1\n",
    "\n",
    "                    else:\n",
    "                        wrong += 1\n",
    "                        if pred == 0 and truth == 1: # if false negative, check if ngs delivery failed\n",
    "                            if ngs_success == False: # if ngs delivery failed\n",
    "                                # y_true[-1] = 0\n",
    "                                failed_bc_dispense += 1\n",
    "                            elif ngs_success == True: # if ngs delivery successful then it must mean that there are beads\n",
    "                                true_false_neg_counter += 1\n",
    "                else:\n",
    "                    unsure.append([row, col])\n",
    "        accuracy_list.append(right / (right + wrong + len(unsure)))\n",
    "        # unsure_dict[filename] = len(unsure)\n",
    "    # print(\"unsure: \", unsure)\n",
    "    # return accuracy_list, y_true, y_pred, wells, totals, specials, with_label, no_label, true_false_neg_counter, failed_bc_dispense/ngs_failed_counter, failed_not_dc_dispense/ngs_failed_counter, ngs_success_counter, ngs_failed_counter, ngs_nan #, unsure_dict\n",
    "    return accuracy_list, y_true, y_pred, with_label, no_label, true_false_neg_counter, failed_bc_dispense/ngs_failed_counter, failed_not_dc_dispense/ngs_failed_counter, ngs_success_counter, ngs_failed_counter, ngs_nan #, unsure_dict\n",
    "    # return None\n",
    "    # return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs, _, _, _, _, true_false_neg_counter, failed_bc_dispense, failed_not_dc_dispense, ngs_success_counter, ngs_failed_counter, ngs_nan = melodys_pipeline(cv2.THRESH_BINARY, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 5, 4, k=9, registration=False)\n",
    "print(np.mean(accs))\n",
    "plt.hist(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_false_neg_counter, failed_bc_dispense, failed_not_dc_dispense, ngs_success_counter, ngs_failed_counter, ngs_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngs_nan/(ngs_success_counter + ngs_failed_counter + ngs_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
